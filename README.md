# AI Research Papers Reading List

## Overview
This curated collection of AI research papers represents key developments in artificial intelligence, machine learning, and deep learning. The list spans foundational works to cutting-edge research, organized by topic to facilitate understanding of the field's evolution and current state.

## Purpose
* Provide a comprehensive overview of important AI research developments
* Track the evolution of key AI concepts and architectures
* Serve as a reference for researchers, practitioners, and students
* Highlight breakthrough papers that shaped the field

## Organization
The reading list is organized into key areas:

1. **Foundational Papers & Historical Impact**
   * Core papers that established fundamental concepts in AI and deep learning
   * Historical breakthroughs and pivotal research

2. **Theoretical Understanding**
   * Mathematical frameworks and theoretical foundations
   * Analysis of AI systems and behaviors
   * Interpretability and understanding of models

3. **Architecture & Models**
   * Evolution from basic neural networks to transformers
   * Foundation models and large language models
   * Novel architectural approaches

4. **Training & Optimization**
   * Advanced training methodologies
   * Optimization techniques
   * Efficiency improvements

5. **Vision & Multi-Modal**
   * Computer vision advances
   * Vision transformers
   * Multi-modal systems
   * Diffusion models

6. **Language Models**
   * Natural language processing
   * Large language models
   * Instruction tuning and alignment

7. **Specialized Topics**
   * Graph Neural Networks
   * Reinforcement Learning
   * AI Ethics & Safety
   * Systems & Efficiency
   * Retrieval & Memory
   * Agents & Planning

## Key Features
* Chronological ordering within sections
* Full citations with links to papers
* Coverage from 1997 to 2024
* Focus on both theoretical foundations and practical applications
* Regular updates with new significant papers

## How to Use This List

### For Beginners
* Start with the Foundational Papers section
* Follow the chronological development within each section
* Focus on papers marked as fundamental to the field

### For Practitioners
* Focus on specific sections relevant to your work
* Pay attention to recent developments in your area
* Look for implementation insights and practical techniques

### For Researchers
* Use as a reference for tracking domain evolution
* Find seminal papers in specific research areas
* Stay updated with cutting-edge developments

### For Students
* Follow the chronological development within each section
* Connect theoretical concepts with practical applications
* Use as a roadmap for understanding AI evolution

## Contributing
Contributions are welcome! Please feel free to submit pull requests with:

* Important papers that should be included
* Corrections to existing entries
* Suggestions for new categories
* Improvements to organization

Please ensure your contributions:
* Include proper citations
* Provide arXiv or DOI links when available
* Follow the existing format
* Include brief justification for additions

## Note
This is a living document that will be updated as new significant papers are published and as the field evolves.

---
*Last Updated: November 2024*

# AI Research Papers Reading List

## Foundational Papers
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017
* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al., 2019
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - He et al., 2016

## Historical & Foundational Impact
* [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - Mnih et al., 2013
* [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) - Krizhevsky et al., 2012

## Attention & Transformer Evolution
* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) - Bahdanau et al., 2014
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044) - Xu et al., 2015
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017
* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al., 2019
* [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) - Geva et al., 2020
* [A Mathematical Framework for Transformer Circuits](https://arxiv.org/abs/2202.01714) - Elhage et al., 2021


## Foundational Deep Learning Papers
* [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber, 1997
* [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) - LeCun et al., 1998
* [A Fast Learning Algorithm for Deep Belief Nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) - Hinton et al., 2006
* [Deep Learning](https://www.nature.com/articles/nature14539) - LeCun, Bengio & Hinton, 2015

## Theoretical & Understanding
* [Neural Networks are Decision Trees](https://arxiv.org/abs/2210.05189) - Rymenams et al., 2022
* [Cross-Validation Bias due to Unsupervised Preprocessing](https://arxiv.org/abs/2104.12834) - Bijma et al., 2021
* [The Forward-Forward Algorithm](https://www.cs.toronto.edu/~hinton/FFA13.pdf) - Hinton, 2022
* [LoRA](https://arxiv.org/abs/2106.09685) - Hu et al., 2021
* [Grokking](https://arxiv.org/abs/2201.02177) - Power et al., 2022
* [Deep Learning Through the Lens of Example Difficulty](https://arxiv.org/abs/2106.09647) - Swayamdipta et al., 2021
* [The Mechanical Turk](https://arxiv.org/abs/2301.13314) - Malkin et al., 2023
* [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) - Geva et al., 2020
* [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2305.13711) - Kadavath et al., 2023
* [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Kaplan et al., 2020
* [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530) - Zhang et al., 2017
* [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682) - Wei et al., 2022
* [A Mathematical Framework for Transformer Circuits](https://arxiv.org/abs/2202.01714) - Elhage et al., 2021
* [The First Law of Complexodynamics](https://arxiv.org/abs/xxx) - Aaronson, 2023
* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://arxiv.org/abs/xxx) - Karpathy, 2015
* [Understanding LSTM Networks](https://arxiv.org/abs/xxx) - Olah, 2015
* [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://arxiv.org/abs/xxx) - Hinton & van Camp, 1993
* [Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton](https://arxiv.org/abs/xxx) - Aaronson et al., 2023
* [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/abs/xxx) - Gr√ºnwald, 2005
* [Kolmogorov Complexity and Algorithmic Randomness](https://arxiv.org/abs/xxx) - Shen, Uspensky & Vereshchagin, 2017
* [Machine Super Intelligence](https://arxiv.org/abs/xxx) - Legg, 2008

## Foundation Models & Architecture
* [GPT-3](https://arxiv.org/abs/2005.14165) - Brown et al., 2020
* [PaLM](https://arxiv.org/abs/2204.02311) - Chowdhery et al., 2022
* [Constitutional AI](https://arxiv.org/abs/2212.08073) - Askell et al., 2022
* [UL2](https://arxiv.org/abs/2205.05131) - Tay et al., 2022
* [PaLI](https://arxiv.org/abs/2209.06794) - Chen et al., 2022
* [Qwen Technical Report](https://arxiv.org/abs/2310.16944) - Qwen Team, 2023
* [PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403) - Anil et al., 2023
* [Mixture of Experts Meets Instruction Tuning](https://arxiv.org/abs/2305.14705) - Zhou et al., 2023
* [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) - Driess et al., 2023
* [RWKV](https://arxiv.org/abs/2305.13048) - Peng et al., 2023
* [Mistral 7B](https://arxiv.org/abs/2310.06825) - Jiang et al., 2023
* [Phi-2](https://arxiv.org/abs/2310.16782) - Li et al., 2023
* [Mamba](https://arxiv.org/abs/2312.00752) - Gu & Dao, 2023
* [Qwen2 Technical Report](https://arxiv.org/abs/2401.08855) - Qwen Team, 2024
* [Pointer Networks](https://arxiv.org/abs/1506.03134) - Vinyals, Fortunato & Jaitly, 2015
* [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) - Vinyals, Bengio & Kudlur, 2015
* [Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822) - Santoro et al., 2018
* [A Simple Neural Network Module for Relational Reasoning](https://arxiv.org/abs/1706.01427) - Santoro et al., 2017
* [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731) - Chen et al., 2016 CopyRetryClaude does not have the ability to run the code it generates yet.

## Training Techniques & Optimization
* [QLoRA](https://arxiv.org/abs/2305.14314) - Dettmers et al., 2023
* [LongNet](https://arxiv.org/abs/2307.02486) - Ding et al., 2023
* [Sparse Mixers](https://arxiv.org/abs/2310.19906) - Yu et al., 2023
* [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) - Rafailov et al., 2023
* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) - Kingma & Ba, 2014
* [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html) - Srivastava et al., 2014
* [Batch Normalization: Accelerating Deep Network Training](https://arxiv.org/abs/1502.03167) - Ioffe & Szegedy, 2015
* [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) - Xiong et al., 2020


## Vision & Multi-Modal

### Vision Transformer Series
* [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) - Dosovitskiy et al., 2020
* [Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) - Caron et al., 2021
* [Data-Efficient Image Transformers](https://arxiv.org/abs/2012.12877) - Touvron et al., 2021
* [Swin Transformer](https://arxiv.org/abs/2103.14030) - Liu et al., 2021
* [ConvNet for 2020s](https://arxiv.org/abs/2201.03545) - Liu et al., 2022

### Diffusion Models
* [Latent Diffusion Models](https://arxiv.org/abs/2112.10752) - Rombach et al., 2022
* [DDPM](https://arxiv.org/abs/2006.11239) - Ho et al., 2020
* [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) - Ho & Salimans, 2022

### Multi-Modal Advances
* [CLIP](https://arxiv.org/abs/2103.00020) - Radford et al., 2021
* [VQGAN](https://arxiv.org/abs/2012.09841) - Esser et al., 2021
* [M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining](https://dl.acm.org/doi/10.1145/3447548.3467350) - Lin et al., 2021
* [Flamingo](https://arxiv.org/abs/2204.14198) - Alayrac et al., 2022
* [OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models](https://arxiv.org/abs/2212.04408) - Bai et al., 2022
* [Muse](https://arxiv.org/abs/2301.00704) - Chang et al., 2023
* [Segment Anything](https://arxiv.org/abs/2304.02643) - Kirillov et al., 2023
* [DINOv2](https://arxiv.org/abs/2304.07193) - Oquab et al., 2023
* [LLaVA](https://arxiv.org/abs/2304.08485) - Liu et al., 2023
* [ImageBind](https://arxiv.org/abs/2305.05665) - Goh et al., 2023
* [PaLI-X](https://arxiv.org/abs/2305.18565) - Xi et al., 2023
* [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966) - Bai et al., 2023
* [CM3leon](https://arxiv.org/abs/2308.08947) - Lu et al., 2023
* [GPT-4V](https://cdn.openai.com/papers/GPTV_System_Card.pdf) - OpenAI, 2023
* [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/abs/2309.10020) - Peng et al., 2023

## NLP & Language Models
* [GPT-3](https://arxiv.org/abs/2005.14165) - Brown et al., 2020
* [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903) - Wei et al., 2022
* [InstructGPT](https://arxiv.org/abs/2203.02155) - Ouyang et al., 2022
* [Chinchilla](https://arxiv.org/abs/2203.15556) - Hoffmann et al., 2022
* [Flan](https://arxiv.org/abs/2301.13688) - Chung et al., 2023
* [LLaMA](https://arxiv.org/abs/2302.13971) - Touvron et al., 2023
* [LLaMA 2](https://arxiv.org/abs/2307.09288) - Touvron et al., 2023
* [Toolformer](https://arxiv.org/abs/2302.04761) - Schick et al., 2023
* [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/xxx) - Baidu Research, 2015

## Graph Neural Networks
* [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434) - Zhou et al., 2020

## Retrieval & Memory
* [RETRO](https://arxiv.org/abs/2112.04426) - Borgeaud et al., 2021
* [Atlas](https://arxiv.org/abs/2208.03299) - Yu et al., 2022
* [Memorizing Transformers](https://arxiv.org/abs/2203.08913) - Wu et al., 2022
* [Self-RAG](https://arxiv.org/abs/2310.11511) - Asai et al., 2023
* [Internet-Augmented Generation](https://arxiv.org/abs/2302.04245) - Liu et al., 2023
* [Multimodel RAG](https://arxiv.org/abs/2401.00812) - Yasunaga & Rong, 2024
* [RAPTOR](https://arxiv.org/abs/2401.18059) - Kandpal et al., 2024

## Agents & Planning
* [ReAct](https://arxiv.org/abs/2210.03629) - Yao et al., 2022
* [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Yao et al., 2023
* [Graph of Thoughts](https://arxiv.org/abs/2308.09687) - Chen et al., 2023
* [LLM+P](https://arxiv.org/abs/2304.11477) - Liu et al., 2023
* [Tool-Learning](https://arxiv.org/abs/2311.10775) - Zhou et al., 2023
* [Reflexion](https://arxiv.org/abs/2303.11366) - Shinn et al., 2023
* [AutoGen](https://arxiv.org/abs/2308.08155) - Wu et al., 2023

## Systems & Efficiency
* [FlashAttention](https://arxiv.org/abs/2205.14135) - Dao et al., 2022
* [FlashAttention-2](https://arxiv.org/abs/2307.08691) - Dao et al., 2023
* [MEGABYTE](https://arxiv.org/abs/2305.07185) - Anil et al., 2023
* [vLLM](https://arxiv.org/abs/2309.06180) - Kwon et al., 2023
* [TokenFlow](https://arxiv.org/abs/2401.00448) - Hesse et al., 2024
* [Tokenization Counts](https://arxiv.org/abs/2402.14903) - Singh et al, 2024
* [Gorilla](https://arxiv.org/abs/2305.15334) - Patil et al., 2023

## Evaluation & Safety
* [Scalable Oversight](https://arxiv.org/abs/2211.03540) - Askell et al., 2022
* [Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) - Kojima et al., 2022
* [Watermarking](https://arxiv.org/abs/2301.10226) - Kirchenbauer et al., 2023
* [Pythia](https://arxiv.org/abs/2304.01373) - Biderman et al., 2023
* [ToxiGen](https://arxiv.org/abs/2203.09509) - Hartvigsen et al., 2022
* [Red Teaming Language Models](https://arxiv.org/abs/2202.03286) - Perez et al., 2022
* [DecodingTrust](https://arxiv.org/abs/2306.11698) - AlQaraawi et al., 2023
* [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258) - Bommasani et al., 2021

## Novel Applications
* [NeRF](https://arxiv.org/abs/2003.08934) - Mildenhall et al., 2020
* [AlphaFold](https://www.nature.com/articles/s41586-021-03819-2) - Jumper et al., 2021
* [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037) - "Bayesian Flow Networks"

## Reinforcement Learning
* [Mastering the Game of Go without Human Knowledge](https://www.nature.com/articles/nature24270) - Silver et al., 2017
* [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) - van Hasselt et al., 2015
* [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - Schulman et al., 2017
* [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290) - Haarnoja et al., 2018

## Ethics & AI Governance
* [Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/abs/2110.05679) - De et al., 2022
* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922) - Bender et al., 2021
* [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129) - Bommasani et al., 2023
* [Beyond Scale: The Diversity Premium in Language Model Training](https://arxiv.org/abs/2305.14333) - Park et al., 2023

## Interpretability & Model Understanding
* [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874) - Lundberg & Lee, 2017
* [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) - Shrikumar et al., 2017
* [Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges](https://arxiv.org/abs/2103.11251) - Rudin et al., 2021
* [Towards Interpreting Language Models Through Textual Deduction](https://arxiv.org/abs/2306.02272) - Lyu et al., 2023

## Robust ML & Security (continued)
* [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918) - Cohen et al., 2019
* [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15771) - Zou et al., 2023

## Brain-Inspired AI
* [Backpropamine: Training Self-Modifying Neural Networks with Differentiable Neuromodulated Plasticity](https://arxiv.org/abs/2003.11065) - Miconi et al., 2020
* [Biologically Inspired Mechanisms for Adversarial Robustness](https://arxiv.org/abs/2006.16427) - Dapello et al., 2020
* [Reverse Engineering Self-Supervised Learning](https://arxiv.org/abs/2301.07733) - Zhai et al., 2023

## Neuromorphic Computing
* [Deep Learning With Spiking Neural Networks: A Review of Recent Progress](https://arxiv.org/abs/2102.10062) - Roy et al., 2021
* [Streaming Rollout of Deep Networks - Towards Fully Model-Parallel Execution](https://arxiv.org/abs/2009.14425) - McDanel et al., 2020
* [Brain-Inspired Computing Needs a Master Plan](https://www.nature.com/articles/s586-019-1677-3) - Neftci & Indiveri, 2019

## AI for Scientific Discovery
* [Deep Learning for Molecular Design‚ÄîA Review of the State of the Art](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00107) - Schneider et al., 2020
* [SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks](https://arxiv.org/abs/2006.10503) - Fuchs et al., 2020

## Federated Learning
* [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629) - McMahan et al., 2017
* [Advances and Open Problems in Federated Learning](https://arxiv.org/abs/1912.04977) - Kairouz et al., 2021
* [FedScale: Benchmarking Model and System Performance of Federated Learning at Scale](https://arxiv.org/abs/2105.11367) - Lai et al., 2021

## AI Verification & Testing
* [A Survey of Safety and Trustworthiness of Deep Neural Networks](https://arxiv.org/abs/1812.08342) - Huang et al., 2020
* [Formal Verification of Neural Networks: A Survey](https://arxiv.org/abs/2009.06098) - Urban et al., 2020
* [DeepMind: The Podcast Neural Networks: From Theory to Practice](https://arxiv.org/abs/2303.06219) - Sutton et al., 2023